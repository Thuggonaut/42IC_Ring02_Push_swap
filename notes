THE CONCEPT OF COMPLEXITY:

Algorithm Analysis: 
The concept of algorithm analysis are step-by-step instructions for solving problems, and analyzing them helps us understand their performance.

Algorithm Efficiency: 
Efficiency is a crucial factor in algorithm analysis. It refers to how quickly an algorithm can solve a problem and how much computational resources it uses, such as time and memory.

Asymptotic Notation: 
This is a mathematical way to describe the behavior of algorithms as their input size becomes very large. Notations like Big O, Omega, and Theta are used to classify the upper and lower bounds of an algorithm's running time.

Time Complexity: 
This refers to the amount of time an algorithm takes to complete as a function of its input size. It's often expressed using Big O notation. Different cases like best-case, worst-case, and average-case time complexities are discussed.

Space Complexity: 
This is the amount of memory space an algorithm uses as a function of its input size. Like time complexity, it's also analyzed using Big O notation.

Types of Analysis: 
There are different ways to analyze algorithms, including worst-case analysis, average-case analysis, amortized analysis, and probabilistic analysis.

Sorting Algorithms: 
Sorting algorithms are used to arrange elements in a specific order. Sorting algorithms include Bubble Sort, Merge Sort, and Quick Sort, and analyzes their time complexities.

Searching Algorithms: 
Searching algorithms such as linear search and binary search are used to find a particular element in a collection of data. 

Dynamic Programming: 
This is a technique used to solve problems by breaking them down into smaller subproblems and reusing solutions to those subproblems.

Graph Algorithms: 
Algorithms for working with graphs include depth-first search (DFS) and breadth-first search (BFS).

Amortized Analysis: 
This technique deals with the average cost per operation in a sequence of operations. It helps understand the overall performance of algorithms over time.

Randomized Algorithms: 
Algorithms that use randomness to improve efficiency are discussed. These algorithms often have probabilistic analysis due to their non-deterministic nature.


MANDATORY PART:



WHAT ARE THE BIG 0, OMEGA AND THETA NOTATIONS?

WHAT ARE THE WORST CASE, AVERAGE CASE, AMORTIZED AND PROBABILISTIC ANALYSIS?

WHAT ARE BUBBLE, MERGE AND QUICK SORTS?

WHAT ARE LINEAR AND BINARY SEARCH ALGORITHMS?

WHAT ARE DFS AND BFS?



//////////////
	•	We might have an application on our phones that store our contacts, with their names and phone numbers sorted alphabetically. The old-school equivalent might be a phone book, a printed copy of names and phone numbers. 
	•	We might open the book and start from the first page, looking for a name one page at a time. This algorithm would be correct, since we will eventually find the name if it’s in the book. 
	•	We might flip through the book two pages at a time, but this algorithm will not be correct since we might skip the page with our name on it. 
	•	Another algorithm would be opening the phone book to the middle, decide whether our name will be in the left half or right half of the book (because the book is alphabetized), and reduce the size of our problem by half. We can repeat this until we find our name, dividing the problem in half each time. 
 
	•	We can visualize the efficiency of each of those algorithms with a chart: 

￼

	•	Our first algorithm, searching one page at a time, can be represented by the red line: our time to solve increases linearly as the size of the problem increases. 
	•	n is a number representing the size of the problem, so with n pages in our phone books, we have to take up to n steps to find a name. 
	•	There's a 1 to 1 relationship between pages and tears. 
	•	The second algorithm, searching two pages at a time, can be represented by the yellow line: our slope is less steep, but still linear. Now, we only need (roughly) n / 2 steps, since we flip two pages at a time. 
	•	Twice as fast, but still the same shape.  
	•	Indicating n/2 maybe + 1 if we have to double back, but fundamentally, still the same algorithm, 1 - 2 pages at a time.  
	•	Our final algorithm, dividing the phone book in half each time, can be represented by the green line, with a fundamentally different relationship between the size of the problem and the time to solve it. If the phone book doubled in size from 1000 to 2000 pages, we would only need one more step to find our name. 
	•	Log arithmetic, in terms of how slow or fast.  
	•	The implication of this algorithm is we could even double the size of the phone book, and no big deal, one additional page tear, and we take yet another 1000 page bite out of the phone book.  


Big O Notation (O):

Definition: 
Big O notation, often denoted as O(f(n)), is used to describe the upper bound or worst-case time complexity of an algorithm. It characterizes how the running time or space requirements of an algorithm grow in relation to the input size (n).

Example: 
If an algorithm has a time complexity of O(n^2), it means that its execution time grows quadratically with the size of the input.


Omega Notation (Ω):

Definition: 
Omega notation, represented as Ω(f(n)), is used to describe the lower bound or best-case time complexity of an algorithm. It provides information about the minimum time or space required for an algorithm to perform a task.

Example: 
If an algorithm has a time complexity of Ω(n), it means that, in the best-case scenario, its execution time grows linearly with the input size.


Theta Notation (Θ):

Definition: 
Theta notation, denoted as Θ(f(n)), provides a tight bound on the time or space complexity of an algorithm. It indicates that the algorithm's performance is neither worse nor better than a specific function of the input size.

Example: 
If an algorithm has a time complexity of Θ(n), it means that its execution time grows linearly with the input size in both the best-case and worst-case scenarios.
In summary, these notations help computer scientists analyze and compare the efficiency of algorithms by providing insights into their behavior under different conditions. Big O represents the upper limit, Omega represents the lower limit, and Theta provides a balanced description of an algorithm's performance. These notations are crucial in selecting the right algorithm for a given problem and understanding how it will scale with larger inputs.




////////////

Oceano: https://www.youtube.com/watch?v=OaG81sDEpVk

Task:
```
Input:					->			Program:		->			Output:
integer arguments					push_swap					list of instructions (seperated by a `\n`)
```

For example:
Input:
 `./push_swap 2 1 34 21 90 -42`
Output:
```
pb
pb
pb
rra
rra
pa
rra
pa
ra
pa
rra
rra
```

Key: Data structures; Algorithms

Header file contains:
- include headers
- data structure for the stack node
- protoypes to create the stack;
- prototypes to handle errors
- prototypes for linked list utils (?)
- prototypes for sorting algorithms
- push swap commands

What is a stack?
- A series of elements stacked on top of each other. 
- Pushing an element on top of a stack, makes that element the "top".
- Removing the top element is "Popping"

We need to implement in our algorithm, a stack:
- An option is to use the "linked list" data structure which consists of nodes.
- Flexible, easy to manipulate, though, complicated to build and easier to seg fault.
- We take every string input argument (the integers) and put each inside a node.
- For example:

`./push_swap "1337 42 -16"`

```

---------
| 1337	|		<- Stack Top node
---------
	|| 
---------
|  42	|		
---------
	||
---------
|  -16	|		<- Stack bottom node
---------
	||
   NULL
```

What is a node?
- A struct.
- Like a "container" that contains some data.
- Like an array, however the difference is, you cannot put different element types inside an array. For instance, inside an array, it has to be all integers, or, all chars, not mixed. 
- A struct on the contrary, can contain different element types, for example, a "struct person" can contain a name, a phone number, etc. 
- In our case, a struct/node contains a value, the top node is `1337`
	- And contains its current `position` which is `0`
		- This is a "meta-data" and every node has it, in order to "make movements"
- Every node has a pointer to its previous node, and a pointer to its' next node.
	- This allows the creation of a "linked list" - a series of nodes, chained to each other inside the heap. 

A node in practice:
- Refer to `push_swap.h` for the blueprint of the struct.
- Refer to the `main.c` where the pointers to the `t_stack_node` structs are declared.
- We use the nodes to implement our stack. 

Command implementations to move our nodes around:
- swap
- rotate
- reverse rotate
- push

swap:
- swaps the top 2 nodes in the stack

```
a			->			a
|						|
1337					42
|						|
42						1337
|						|
-16						-16
|						|
NULL					NULL
```

rotate:
- rotates the top node to the bottom of the stack
- shifting the remaining nodes towards the top of the stack

```
a			->			a
|						|
42						1337
|						|
1337					-16
|						|
-16						42
|						|
NULL					NULL
```

reverse rotate:
- rotates the bottom node to the top of the stack
- shifting the remaing nodes towards the bottom of the stack

```
a			->			a
|						|
1337					42
|						|
-16						1337
|						|
42						-16
|						|
NULL					NULL
```

push:
- "pops" off the top node of one stack
- places it on top of another stack - another linked list, an auxiliary stack

```
a			&			b			->				a			&			b
|						|							|						|
42						21							1337					42	
|						|							|						|
1337					NULL						-16						21
|													|						|
-16													NULL					NULL
|						
NULL				
```


Now that we have our commands implemented, we need to implement the algorithms:

tiny_sort:
- The base case. Whenever we have 3 nodes in a stack, we use tiny_sort.
- The idea that, if there are 3 nodes, it'll be very easy to sort:
	- We ensure the biggest is at the bottom.
	- Then swap the first two nodes if needed.

The complications come when we have more than 3 nodes to sort in the least amount of moves.

Consider the following example. We push all the stack `a` nodes onto stack `b` until only 3 nodes remain in `a`

```
a			&			b			->				a			&			b
|						|							|						|
1337					NULL						-21						42
|													|						|
42													96						1337
|													|						|
-21													-16						NULL
|													NULL
96
|
-16
|
NULL
```

We've now reached our base case, and we perfom tiny_sort on stack `a`:

```
a			->			a
|						|
-21						-21
|						|
96						-16
|						|
-16						96
|						|
NULL					NULL
```

```
a			&			b
|						|
-21						42
|						|
-16						1337
|						|
96						NULL
|						
NULL					
```

Now we need to perfom a precise insertion of the node from `b` to `a`: 
- Every node in `b` gets a `target_node` in `a`, that is bigger, but also the best fit. 
	- In other words, each node in `b` has to match with a node in `a`.
	- The `a` `target_node` has to be the best bigger node (the smallest - bigger node).
		- For example, above, `the target_node` of `42` would be `96`, because it is bigger than `42`, but it is the "smallest bigger".
	- If there are none, then the target is the smallest node.
		- For example, above, there is no number bigger than `1337` in stack `a`, so, we search for the smallest number, and the `target_node` then, becomes `-21`.
	- The `target_node` in stack `a` is what we want to push the current node from stack `b`, on top of.
	- We can imagine the stacks as a circular linked list.
		- Which is why, if a node from stack `b` has no bigger number in stack `a`, the target is the smallest from stack `a`. 

Now that we have all the target_nodes, we ask, which `b` node is the `cheapest` to move?
- Recall, every node has its `current_position` occupied in the stack, which needs to be refreshed after each move.
- So, we need to assign all the nodes from both stacks, their `current_position` after each move.
- How do we find which `b` node is the `cheapest` to move?
	- To get a `push_price` value, we sum the positions of:
		- `b->current_position` `+`
		- `target_node->current_position`
	- How many moves to bring the `b` node to the top, `+` how many moved to bring `a` node to the top?
		- The cost is the sum of moves for both `b` and `a`.


////////////////////

Jamie Dawson https://medium.com/@jamierobertdawson/push-swap-the-least-amount-of-moves-with-two-stacks-d1e76a71789a

- actions (commands)
- handle 3:
	- smallest on top
	- biggest on bottom
- handle 5:
	- push top two nodes from `a` to `b`
	- apply handle 3 logic to stack a
	- ensure the correct order in stack a, before accepting `b` nodes
- handle 100:
	- split 100 into 5 chunks of 20
	- 


A. Yigit Ogun https://medium.com/@ayogun/push-swap-c1f5d2d41e97


https://medium.com/geekculture/a-beginners-guide-to-setup-opengl-in-linux-debian-2bfe02ccd1e
https://gist.github.com/shamiul94/a632f7ab94cf389e08efd7174335df1c
install OpenGL
chat gpt'd all lib errors
permission denied:
chmod +x ../../push_swap
